{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954952b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: feedparser in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (6.0.11)\n",
      "Requirement already satisfied: pymongo in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.10.1)\n",
      "Requirement already satisfied: rapidfuzz in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: selenium in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.25.0)\n",
      "Requirement already satisfied: transformers in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: torch in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: sgmllib3k in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: filelock in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: sympy in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from urllib3<3,>=1.21.1->requests) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 feedparser pymongo rapidfuzz selenium transformers torch\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from rapidfuzz import fuzz\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from transformers import pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ec0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of RSS feed URLs\n",
    "rss_feeds = {\n",
    "    \"BBC\": \"http://feeds.bbci.co.uk/news/rss.xml\",\n",
    "    \"CNN\": \"http://rss.cnn.com/rss/edition.rss\",\n",
    "    \"Reuters\": [\n",
    "        \"https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=business-finance&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=deals&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=political-general&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=environment&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=tech&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=health&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=sports&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=lifestyle-entertainment&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=human-interest&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=journalist-spotlight&post_type=best\",\n",
    "    ],\n",
    "    \n",
    "    \"TOI\": \"https://timesofindia.indiatimes.com/rssfeedstopstories.cms\", #The New York Times\n",
    "    #\"TNYT\": \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\", #The New York Times\n",
    "    #\"TG\": \"https://www.theguardian.com/uk/rss\", #The Guardian\n",
    "    #\"AJ\": \"https://www.aljazeera.com/xml/rss/all.xml\" #Al Jazeera\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386da59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Setup\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Update with your MongoDB connection URI\n",
    "db = client['news_database']\n",
    "collection = db['news_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23dcae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Load a summarization model and sentiment analysis model\n",
    "summarizer = pipeline('summarization')\n",
    "sentiment_analyzer = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2478417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # Runs Chrome in headless mode\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service('/opt/homebrew/bin/chromedriver')  # Update this path if necessary\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f579a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relative_time_to_iso(relative_time):\n",
    "    \"\"\"\n",
    "    Convert a relative time string like '21 hours ago' to an ISO timestamp.\n",
    "    \"\"\"\n",
    "    # Get the current time\n",
    "    current_time = datetime.utcnow()\n",
    "\n",
    "    # Use regular expressions to extract time units and values\n",
    "    match = re.match(r\"(\\d+)\\s*(\\w+)\\s*ago\", relative_time)\n",
    "    if not match:\n",
    "        return None  # Return None if the format is not recognized\n",
    "\n",
    "    amount, unit = int(match.group(1)), match.group(2)\n",
    "\n",
    "    # Convert the time unit into a timedelta object\n",
    "    if \"hour\" in unit:\n",
    "        time_delta = timedelta(hours=amount)\n",
    "    elif \"minute\" in unit:\n",
    "        time_delta = timedelta(minutes=amount)\n",
    "    elif \"second\" in unit:\n",
    "        time_delta = timedelta(seconds=amount)\n",
    "    elif \"day\" in unit:\n",
    "        time_delta = timedelta(days=amount)\n",
    "    else:\n",
    "        return None  # If it's a unit we don't handle, return None\n",
    "\n",
    "    # Subtract the timedelta from the current time\n",
    "    calculated_time = current_time - time_delta\n",
    "\n",
    "    # Convert to ISO format\n",
    "    return calculated_time.isoformat() + \"Z\"  # Adding 'Z' to denote UTC time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8080151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cnn_time(cnn_time):\n",
    "    # Step 1: Remove \"Updated\" text\n",
    "    cnn_time = cnn_time.replace(\"Updated\", \"\").strip()\n",
    "\n",
    "    # Step 2: Parse the time (assuming it's in Eastern Daylight Time - EDT)\n",
    "    # We'll remove the EDT part and parse the rest of the time\n",
    "    time_format = \"%I:%M %p %Z, %a %B %d, %Y\"\n",
    "    cnn_time = cnn_time.replace(\"EDT\", \"Eastern\")  # Handle EDT as \"Eastern\" for parsing\n",
    "    dt = datetime.strptime(cnn_time, time_format)\n",
    "    \n",
    "    # Convert to UTC or desired timezone (for example, GMT)\n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    dt_eastern = eastern.localize(dt)  # Localize the datetime to Eastern Time\n",
    "    dt_utc = dt_eastern.astimezone(pytz.utc)  # Convert to UTC\n",
    "\n",
    "    # Step 3: Convert to desired format (similar to BBC format)\n",
    "    return dt_utc.strftime('%d %b %Y, %H:%M GMT')\n",
    "\n",
    "# Test with your input example\n",
    "#cnn_published_time = \"Updated  4:09 PM EDT, Thu April 6, 2023\"\n",
    "#formatted_time = convert_cnn_time(cnn_published_time)\n",
    "#print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb32a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch full news content and additional details from the news article page\n",
    "def fetch_full_article_bbc(url, driver):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        #time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Initialize fields\n",
    "        full_text = \"\"\n",
    "        published_time = \"\"\n",
    "        published_time_raw = \"\"\n",
    "        author_name = \"\"\n",
    "        author_designation = \"\"\n",
    "        reporting_location = \"\"\n",
    "        images = []\n",
    "\n",
    "        # Extract the article content\n",
    "        article = soup.find('article')\n",
    "        if article:\n",
    "            # Extract paragraphs\n",
    "            paragraphs = article.find_all('p')\n",
    "            full_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "            #print(f\"Full text: {full_text}\")\n",
    "\n",
    "            # Extract published time\n",
    "            time_tag = article.find('time')\n",
    "            if time_tag:\n",
    "                published_time_raw = time_tag.get_text()                \n",
    "                print(f\"Time: {published_time_raw}\")\n",
    "                published_time = convert_relative_time_to_iso(published_time_raw)\n",
    "\n",
    "\n",
    "            # Extract author name and designation\n",
    "            byline_block = article.find('div', {'data-component': 'byline-block'})\n",
    "            if byline_block:\n",
    "                author_name_tag = byline_block.find('span', class_='bZCrck')\n",
    "                if author_name_tag:\n",
    "                    author_name = author_name_tag.get_text()\n",
    "\n",
    "                author_designation_tag = byline_block.find('div', class_='hEbjLr')\n",
    "                if author_designation_tag:\n",
    "                    author_designation = author_designation_tag.get_text()\n",
    "\n",
    "                # Extract reporting location\n",
    "                reporting_location_tag = byline_block.find('span', string=lambda x: x and \"Reporting from\" in x)\n",
    "                if reporting_location_tag:\n",
    "                    reporting_location = reporting_location_tag.get_text().replace(\"Reporting from\", \"\").strip()\n",
    "        \n",
    "        # Extract images within <figure> tags\n",
    "        figures = soup.find_all('figure')\n",
    "        for fig in figures:\n",
    "            img = fig.find('img')\n",
    "            if img and img.get('src'):\n",
    "                # Some images might have relative URLs\n",
    "                img_url = img['src']\n",
    "                if img_url.startswith('//'):\n",
    "                    img_url = 'https:' + img_url\n",
    "                elif img_url.startswith('/'):\n",
    "                    img_url = 'https://www.bbc.com' + img_url\n",
    "                images.append(img_url)\n",
    "        \n",
    "        return {\n",
    "            \"description\": full_text if full_text else \"Full content not available\",\n",
    "            \"published_time\": published_time,\n",
    "            \"author_name\": author_name,\n",
    "            \"author_designation\": author_designation,\n",
    "            \"reporting_location\": reporting_location,\n",
    "            \"images\": images\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch full content from {url}: {e}\")\n",
    "        return {\n",
    "            \"description\": \"Failed to fetch full content\",\n",
    "            \"published_time\": \"\",\n",
    "            \"author_name\": \"\",\n",
    "            \"author_designation\": \"\",\n",
    "            \"reporting_location\": \"\",\n",
    "            \"images\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e2a308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the full Reuters article data\n",
    "def fetch_full_article_reuters(rss_article_url, driver):\n",
    "    try:\n",
    "        # Step 1: Navigate to the RSS article page\n",
    "        # Simulate mouse movement\n",
    "        action = ActionChains(driver)\n",
    "        element = driver.find_element_by_xpath(\"//button[@class='submit']\")\n",
    "        action.move_to_element(element).perform()\n",
    "\n",
    "        # Randomize delay\n",
    "        time.sleep(2 + random.random() * 3)  # Pause 2 to 5 seconds\n",
    "\n",
    "        # Click the button to trigger a click event manually\n",
    "        element.click()\n",
    "\n",
    "        driver.get(rss_article_url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Step 2: Find the actual article link from the RSS article page\n",
    "        article_link_tag = soup.find('a', href=True, text=\"exclusively reported\")\n",
    "        \n",
    "        if not article_link_tag:\n",
    "            print(f\"Actual article link not found on {rss_article_url}\")\n",
    "            return {\"description\": \"Failed to fetch full content\", \"author_name\": \"\", \"category\": \"\", \"headline\": \"Headline not found\"}\n",
    "\n",
    "        article_url = article_link_tag['href']\n",
    "        print(f\"Found article link: {article_url}\")\n",
    "        \n",
    "        # Step 3: Navigate to the actual article page\n",
    "        driver.get(article_url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Step 4: Extract title\n",
    "        title_tag = soup.find('h1', {'data-testid': 'Heading'})\n",
    "        headline = title_tag.get_text(strip=True) if title_tag else 'Headline not found'\n",
    "        print(f\"Title: {headline}\")\n",
    "        \n",
    "        # Step 5: Extract author\n",
    "        author_tag = soup.find('a', rel='author')\n",
    "        author_name = author_tag.get_text(strip=True) if author_tag else 'Author not found'\n",
    "        print(f\"Author: {author_name}\")\n",
    "        \n",
    "        # Step 6: Extract category\n",
    "        category_tag = soup.find('ul', {'class': 'tags-with-tooltip__list__37vkr'})\n",
    "        category = category_tag.get_text(strip=True) if category_tag else 'Category not found'\n",
    "        print(f\"Category: {category}\")\n",
    "        \n",
    "        # Step 7: Extract description/content\n",
    "        description_tag = soup.find('div', {'class': 'article-body__content__17Yit'})\n",
    "        description = description_tag.get_text(strip=True) if description_tag else 'Description not found'\n",
    "        print(f\"Description: {description}\")\n",
    "\n",
    "        # Return the extracted data as a dictionary\n",
    "        return {\n",
    "            'headline': headline,\n",
    "            'author_name': author_name,\n",
    "            'category': category,\n",
    "            'description': description,\n",
    "            'published_time': datetime.now().isoformat(),  # You can replace this with actual fetched timestamp logic\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch full content from {rss_article_url}: {e}\")\n",
    "        return {\n",
    "            \"description\": \"Failed to fetch full content\",\n",
    "            \"author_name\": \"\",\n",
    "            \"category\": \"\",\n",
    "            \"headline\": \"Headline not found\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "475d3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_tag(entry):\n",
    "    \"\"\"Assign a specific tag to an article based on custom logic\"\"\"\n",
    "    if 'breaking' in entry.title.lower():\n",
    "        return 1  # Tag for breaking news\n",
    "    elif 'politics' in entry.title.lower():\n",
    "        return 2  # Tag for COVID-related news\n",
    "    elif 'business' in entry.title.lower():\n",
    "        return 3  # Tag for election news\n",
    "    elif 'science' in entry.title.lower():\n",
    "        return 4  # Tag for election news\n",
    "    elif 'sports' in entry.title.lower():\n",
    "        return 5  # Tag for election news\n",
    "    elif 'arts' in entry.title.lower():\n",
    "        return 6  # Tag for election news\n",
    "    elif 'entertainment' in entry.title.lower():\n",
    "        return 7  # Tag for election news\n",
    "    elif 'travel' in entry.title.lower():\n",
    "        return 8  # Tag for election news\n",
    "    elif 'weather' in entry.title.lower():\n",
    "        return 9  # Tag for election news\n",
    "    elif 'earth' in entry.title.lower():\n",
    "        return 10  # Tag for election news\n",
    "    else:\n",
    "        return 0  # Default tag for other news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5de45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_article_other(new_article, existing_articles):\n",
    "    \"\"\"Check if the new article is a duplicate based on title similarity using rapidfuzz.\"\"\"\n",
    "    \n",
    "    for article_item in existing_articles:\n",
    "        if 'headline' in article_item:\n",
    "            similarity = fuzz.token_set_ratio(new_article['headline'], article_item['headline'])\n",
    "            if similarity > 85:  # Threshold for considering articles as duplicates\n",
    "                print(f\"Duplicate title found: {article_item['headline']}\")\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd38a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_article(new_article, existing_articles):\n",
    "    \"\"\"\n",
    "    Check if the new article is a duplicate based on title similarity and source.\n",
    "    If the title matches exactly and the source is the same, it's considered a duplicate.\n",
    "    \"\"\"\n",
    "    for article_item in existing_articles:\n",
    "        # Ensure the existing article has a headline and source\n",
    "        if 'headline' in article_item and 'source' in article_item:\n",
    "            if article_item['headline'] == new_article['headline'] and article_item['source'] == new_article['source']:\n",
    "                print(f\"Duplicate title found in the same source: {article_item['headline']} (Source: {article_item['source']})\")\n",
    "                return True\n",
    "        else:\n",
    "            # If either headline or source is missing, we can't reliably check for duplicates\n",
    "            continue\n",
    "            \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2955ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_and_save_duplicate_articles():\n",
    "    \"\"\"Mark duplicate articles with the same tagId based on title similarity and save in the DB.\"\"\"\n",
    "    \n",
    "    # Fetch all articles from the collection\n",
    "    articles = list(collection.find())\n",
    "    \n",
    "    current_tag_id = 1  # Starting tagId\n",
    "\n",
    "    # Track tagIds for already tagged groups of duplicates\n",
    "    assigned_tag_ids = {}\n",
    "\n",
    "    for i, new_article in enumerate(articles):\n",
    "        if 'tagId' in new_article and new_article['tagId'] is not None:\n",
    "            continue  # Skip already tagged articles\n",
    "\n",
    "        # If the article is not tagged, assign a new tagId\n",
    "        new_article_tag_id = current_tag_id\n",
    "\n",
    "        # Update the current article with the tagId\n",
    "        new_article['_id']  # Get the ID of the new article\n",
    "        collection.update_one({'_id': new_article['_id']}, {'$set': {'tagId': new_article_tag_id}})\n",
    "\n",
    "        # Compare the current article with the rest to find duplicates\n",
    "        for j in range(i + 1, len(articles)):\n",
    "            existing_article = articles[j]\n",
    "\n",
    "            if 'headline' in existing_article:\n",
    "                similarity = fuzz.token_set_ratio(new_article['headline'], existing_article['headline'])\n",
    "\n",
    "                if similarity > 85:\n",
    "                    print(f\"Duplicate title found: {existing_article['headline']}\")\n",
    "\n",
    "                    # Check if the existing article already has a tagId\n",
    "                    if 'tagId' in existing_article and existing_article['tagId'] is not None:\n",
    "                        # Use the existing article's tagId for the current article group\n",
    "                        new_article_tag_id = existing_article['tagId']\n",
    "                    else:\n",
    "                        # Assign the current tagId to the existing duplicate article\n",
    "                        existing_article['tagId'] = new_article_tag_id\n",
    "                        collection.update_one({'_id': existing_article['_id']}, {'$set': {'tagId': new_article_tag_id}})\n",
    "\n",
    "        # Increment the tagId only if it was a unique article (no duplicates found)\n",
    "        if new_article_tag_id == current_tag_id:\n",
    "            current_tag_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06b8379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_mongo(articles):\n",
    "    \"\"\"Save articles to MongoDB, avoiding duplicates based on title similarity across sources.\"\"\"\n",
    "    # Fetch all existing articles from the database for comparison\n",
    "    existing_articles = list(collection.find({}, {'headline': 1}))  # Only fetch titles for comparison\n",
    "\n",
    "    for article in articles:\n",
    "        # Check for duplicate articles based on title similarity\n",
    "        if not is_duplicate_article(article, existing_articles):\n",
    "            collection.insert_one(article)\n",
    "            print(f\"Saved article: {article['headline']}\")\n",
    "        else:\n",
    "            print(f\"Duplicate article skipped: {article['headline']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78649a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rss_feed(rss_url, driver, portal):\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    \n",
    "    news_data = []\n",
    "    # Fetch existing titles from MongoDB for duplicate checking\n",
    "    existing_titles = list(collection.find({}, {'headline': 1}))\n",
    "    #print(f\"Skipping duplicate titles: {existing_titles}\")\n",
    "    existing_titles = [item['headline'] for item in existing_titles]\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        headline = entry.title\n",
    "        url = entry.link\n",
    "        article_details = ''\n",
    "\n",
    "        # Check for duplicates\n",
    "        if is_duplicate_article({\"headline\": headline}, existing_titles):\n",
    "            print(f\"Duplicate article skipped: {headline}\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        print(f\"Portal: {portal}\")\n",
    "\n",
    "        # Fetch additional details from the article page\n",
    "        if portal == 'bbc':\n",
    "            article_details = fetch_full_article_bbc(url, driver)\n",
    "        elif portal == 'cnn':\n",
    "            article_details = fetch_full_article_cnn(url, driver)\n",
    "        elif portal == 'reuters':\n",
    "            article_details = fetch_full_article_reuters(url, driver)\n",
    "\n",
    "        # Use .get() to avoid KeyError for missing fields\n",
    "        news_item = {\n",
    "            \"headline\": headline,\n",
    "            \"url\": url,\n",
    "            \"description\": article_details.get(\"description\", \"\"),\n",
    "            \"published_time\": article_details.get(\"published_time\", \"\"),\n",
    "            \"author_name\": article_details.get(\"author_name\", \"\"),\n",
    "            \"author_designation\": article_details.get(\"author_designation\", \"\"),  # Optional field\n",
    "            \"reporting_location\": article_details.get(\"reporting_location\", \"\"),  # Optional field\n",
    "            \"images\": article_details.get(\"images\", []),  # Optional field, default to empty list\n",
    "            \"source\": portal,\n",
    "            \"category\": \"Top Stories\",\n",
    "            \"fetched_at\": datetime.now()\n",
    "        }\n",
    "        #print(news_item)  # Print news details to the console\n",
    "        news_data.append(news_item)\n",
    "    \n",
    "    return news_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "523e5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news():\n",
    "    \"\"\"Crawl all news outlets and save to MongoDB\"\"\"    \n",
    "    driver = setup_driver()\n",
    "    rss_url = \"\"\n",
    "    try:\n",
    "        # Handle other portals with single URL feeds\n",
    "        news_data = parse_rss_feed(rss_url, driver, portal.lower().replace(' ', '_'))\n",
    "        save_articles_to_mongo(news_data)\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    crawl_news()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
