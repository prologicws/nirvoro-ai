{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "954952b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: feedparser in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (6.0.11)\n",
      "Requirement already satisfied: pymongo in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.10.1)\n",
      "Requirement already satisfied: rapidfuzz in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: selenium in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.25.0)\n",
      "Requirement already satisfied: transformers in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: sgmllib3k in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: filelock in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: networkx in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from urllib3<3,>=1.21.1->requests) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dibyendu/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/dibyendu/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <F6236B89-E4CA-3330-B665-E463D537EAF3> /Users/dibyendu/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <A51C8C05-245A-3989-8D3C-9A6704422CA5> /Users/dibyendu/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 feedparser pymongo rapidfuzz selenium transformers torch\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from rapidfuzz import fuzz\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from transformers import pipeline\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386da59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Setup\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Update with your MongoDB connection URI\n",
    "db = client['news_database']\n",
    "collection = db['news_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765cbc6-777f-43cc-9c06-8fbc4c43f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a summarization model and sentiment analysis model\n",
    "summarizer = pipeline('summarization')\n",
    "sentiment_analyzer = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2478417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # Runs Chrome in headless mode\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service('/opt/homebrew/bin/chromedriver')  # Update this path if necessary\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f579a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relative_time_to_iso(relative_time):\n",
    "    \"\"\"\n",
    "    Convert a relative time string like '21 hours ago' to an ISO timestamp.\n",
    "    \"\"\"\n",
    "    # Get the current time\n",
    "    current_time = datetime.utcnow()\n",
    "\n",
    "    # Use regular expressions to extract time units and values\n",
    "    match = re.match(r\"(\\d+)\\s*(\\w+)\\s*ago\", relative_time)\n",
    "    if not match:\n",
    "        return None  # Return None if the format is not recognized\n",
    "\n",
    "    amount, unit = int(match.group(1)), match.group(2)\n",
    "\n",
    "    # Convert the time unit into a timedelta object\n",
    "    if \"hour\" in unit:\n",
    "        time_delta = timedelta(hours=amount)\n",
    "    elif \"minute\" in unit:\n",
    "        time_delta = timedelta(minutes=amount)\n",
    "    elif \"second\" in unit:\n",
    "        time_delta = timedelta(seconds=amount)\n",
    "    elif \"day\" in unit:\n",
    "        time_delta = timedelta(days=amount)\n",
    "    else:\n",
    "        return None  # If it's a unit we don't handle, return None\n",
    "\n",
    "    # Subtract the timedelta from the current time\n",
    "    calculated_time = current_time - time_delta\n",
    "\n",
    "    # Convert to ISO format\n",
    "    return calculated_time.isoformat() + \"Z\"  # Adding 'Z' to denote UTC time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch full news content and additional details from the news article page\n",
    "def fetch_full_article_bbc_1(url, driver):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        #time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Initialize fields\n",
    "        full_text = \"\"\n",
    "        published_time = \"\"\n",
    "        published_time_raw = \"\"\n",
    "        author_name = \"\"\n",
    "        author_designation = \"\"\n",
    "        reporting_location = \"\"\n",
    "        images = []\n",
    "\n",
    "        # Extract the article content\n",
    "        article = soup.find('article')\n",
    "        if article:\n",
    "            # Extract paragraphs\n",
    "            paragraphs = article.find_all('p')\n",
    "            full_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "            #print(f\"Full text: {full_text}\")\n",
    "\n",
    "            # Extract published time\n",
    "            time_tag = article.find('time')\n",
    "            if time_tag:\n",
    "                published_time_raw = time_tag.get_text()                \n",
    "                print(f\"Time: {published_time_raw}\")\n",
    "                published_time = convert_relative_time_to_iso(published_time_raw)\n",
    "\n",
    "\n",
    "            # Extract author name and designation\n",
    "            byline_block = article.find('div', {'data-component': 'byline-block'})\n",
    "            if byline_block:\n",
    "                author_name_tag = byline_block.find('span', class_='bZCrck')\n",
    "                if author_name_tag:\n",
    "                    author_name = author_name_tag.get_text()\n",
    "\n",
    "                author_designation_tag = byline_block.find('div', class_='hEbjLr')\n",
    "                if author_designation_tag:\n",
    "                    author_designation = author_designation_tag.get_text()\n",
    "\n",
    "                # Extract reporting location\n",
    "                reporting_location_tag = byline_block.find('span', string=lambda x: x and \"Reporting from\" in x)\n",
    "                if reporting_location_tag:\n",
    "                    reporting_location = reporting_location_tag.get_text().replace(\"Reporting from\", \"\").strip()\n",
    "        \n",
    "        # Extract images within <figure> tags\n",
    "        figures = soup.find_all('figure')\n",
    "        for fig in figures:\n",
    "            img = fig.find('img')\n",
    "            if img and img.get('src'):\n",
    "                # Some images might have relative URLs\n",
    "                img_url = img['src']\n",
    "                if img_url.startswith('//'):\n",
    "                    img_url = 'https:' + img_url\n",
    "                elif img_url.startswith('/'):\n",
    "                    img_url = 'https://www.bbc.com' + img_url\n",
    "                images.append(img_url)\n",
    "        \n",
    "        return {\n",
    "            \"description\": full_text if full_text else \"Full content not available\",\n",
    "            \"published_time\": published_time,\n",
    "            \"author_name\": author_name,\n",
    "            \"author_designation\": author_designation,\n",
    "            \"reporting_location\": reporting_location,\n",
    "            \"images\": images\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch full content from {url}: {e}\")\n",
    "        return {\n",
    "            \"description\": \"Failed to fetch full content\",\n",
    "            \"published_time\": \"\",\n",
    "            \"author_name\": \"\",\n",
    "            \"author_designation\": \"\",\n",
    "            \"reporting_location\": \"\",\n",
    "            \"images\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e2a308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch full news content and additional details from the news article page\n",
    "def fetch_full_article_bbc(url, driver):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        #time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Initialize fields\n",
    "        full_text = \"\"\n",
    "        published_time = \"\"\n",
    "        published_time_raw = \"\"\n",
    "        author_name = \"\"\n",
    "        author_designation = \"\"\n",
    "        reporting_location = \"\"\n",
    "        images = []\n",
    "\n",
    "        # Extract the article content\n",
    "        article = soup.find('article')\n",
    "        if article:\n",
    "            # Extract paragraphs\n",
    "            paragraphs = article.find_all('p')\n",
    "            full_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "            #print(f\"Full text: {full_text}\")\n",
    "\n",
    "            # Extract published time\n",
    "            time_tag = article.find('time')\n",
    "            if time_tag:\n",
    "                published_time_raw = time_tag.get_text()                \n",
    "                print(f\"Time: {published_time_raw}\")\n",
    "                published_time = convert_relative_time_to_iso(published_time_raw)\n",
    "\n",
    "\n",
    "            # Extract author name and designation\n",
    "            byline_block = article.find('div', {'data-component': 'byline-block'})\n",
    "            if byline_block:\n",
    "                author_name_tag = byline_block.find('span', class_='bZCrck')\n",
    "                if author_name_tag:\n",
    "                    author_name = author_name_tag.get_text()\n",
    "\n",
    "                author_designation_tag = byline_block.find('div', class_='hEbjLr')\n",
    "                if author_designation_tag:\n",
    "                    author_designation = author_designation_tag.get_text()\n",
    "\n",
    "                # Extract reporting location\n",
    "                reporting_location_tag = byline_block.find('span', string=lambda x: x and \"Reporting from\" in x)\n",
    "                if reporting_location_tag:\n",
    "                    reporting_location = reporting_location_tag.get_text().replace(\"Reporting from\", \"\").strip()\n",
    "        \n",
    "        # Extract images within <figure> tags\n",
    "        figures = soup.find_all('figure')\n",
    "        for fig in figures:\n",
    "            img = fig.find('img')\n",
    "            if img and img.get('src'):\n",
    "                # Some images might have relative URLs\n",
    "                img_url = img['src']\n",
    "                if img_url.startswith('//'):\n",
    "                    img_url = 'https:' + img_url\n",
    "                elif img_url.startswith('/'):\n",
    "                    img_url = 'https://www.bbc.com' + img_url\n",
    "                images.append(img_url)\n",
    "        \n",
    "        return {\n",
    "            \"description\": full_text if full_text else \"Full content not available\",\n",
    "            \"published_time\": published_time,\n",
    "            \"author_name\": author_name,\n",
    "            \"author_designation\": author_designation,\n",
    "            \"reporting_location\": reporting_location,\n",
    "            \"images\": images\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch full content from {url}: {e}\")\n",
    "        return {\n",
    "            \"description\": \"Failed to fetch full content\",\n",
    "            \"published_time\": \"\",\n",
    "            \"author_name\": \"\",\n",
    "            \"author_designation\": \"\",\n",
    "            \"reporting_location\": \"\",\n",
    "            \"images\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e0c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_article(new_article, existing_articles):\n",
    "    \"\"\"\n",
    "    Check if the new article is a duplicate based on title similarity and source.\n",
    "    If the title matches exactly and the source is the same, it's considered a duplicate.\n",
    "    \"\"\"\n",
    "    for article_item in existing_articles:\n",
    "        # Ensure the existing article has a title and source\n",
    "        if 'title' in article_item and 'source' in article_item:\n",
    "            if article_item['title'] == new_article['title'] and article_item['source'] == new_article['source']:\n",
    "                print(f\"Duplicate title found in the same source: {article_item['title']} (Source: {article_item['source']})\")\n",
    "                return True\n",
    "        else:\n",
    "            # If either title or source is missing, we can't reliably check for duplicates\n",
    "            continue\n",
    "            \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c542b466-270d-4be2-bea1-02437c2dd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all articles from the collection\n",
    "#articles = []\n",
    "#articles = list(collection.find())\n",
    "#mark_and_save_duplicate_articles();\n",
    "\n",
    "def clean_sensationalism(article_text):\n",
    "    \"\"\"\n",
    "    Cleans sensationalism by detecting overly emotional or sensational language \n",
    "    and rephrasing the text to focus on the facts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Break the text into sentences\n",
    "    if not article_text:  # Check if article_text is None or empty\n",
    "        return \"Content not available or cannot be cleaned.\"\n",
    "    \n",
    "    sentences = article_text.split('. ')\n",
    "    factual_sentences = []\n",
    "    print(f\"Sentences: {sentences}\")\n",
    "\n",
    "    # Analyze each sentence\n",
    "    for sentence in sentences:\n",
    "        sentiment = sentiment_analyzer(sentence)\n",
    "        if sentiment[0]['label'] in ['NEGATIVE', 'POSITIVE'] and sentiment[0]['score'] > 0.7:\n",
    "            # Skip overly emotional sentences or rewrite them\n",
    "            print(f\"Skipping sensational sentence: {sentence}\")\n",
    "            continue\n",
    "        factual_sentences.append(sentence)\n",
    "\n",
    "    # Join the factual sentences\n",
    "    clean_text = '. '.join(factual_sentences)\n",
    "    \n",
    "    # Use summarization to condense the cleaned text\n",
    "    summary = summarizer(clean_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30c9a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_category(article_content):\n",
    "    \"\"\"\n",
    "    Determines the category of the article based on its content.\n",
    "    \"\"\"\n",
    "    # Define keywords for each category\n",
    "    categories = {\n",
    "        #\"Breaking\": [\"breaking\", \"urgent\", \"just in\"],\n",
    "        \"World\": [\"government\", \"policy\", \"election\", \"political\", \"politics\", \"weather\", \"storm\", \"temperature\", \"forecast\", \"climate\", \"environment\", \"sustainability\", \"nature\"],\n",
    "        \"Business\": [\"business\", \"market\", \"stocks\", \"finance\", \"economy\"],\n",
    "        \"Technology\": [\"research\", \"scientists\", \"study\", \"laboratory\", \"experiment\", \"science\"],\n",
    "        \"Sports\": [\"game\", \"match\", \"tournament\", \"league\", \"athlete\"],\n",
    "        #\"Art & Culture\": [\"art\", \"museum\", \"culture\", \"festival\", \"heritage\"],\n",
    "        \"Entertainment\": [\"celebrity\", \"movie\", \"music\", \"show\", \"award\", \"art\", \"museum\", \"culture\", \"festival\", \"heritage\", \"travel\", \"tourism\", \"destination\", \"flight\", \"hotel\"],\n",
    "        #\"Travel\": [\"travel\", \"tourism\", \"destination\", \"flight\", \"hotel\"],\n",
    "        #\"Weather\": [\"weather\", \"storm\", \"temperature\", \"forecast\"],\n",
    "        #\"Earth\": [\"climate\", \"environment\", \"sustainability\", \"nature\"],\n",
    "        #\"Local\": [\"local\", \"community\", \"neighborhood\", \"town\"]\n",
    "    }\n",
    "\n",
    "    # Default category\n",
    "    assigned_category = \"World\"\n",
    "\n",
    "    # Check for category keywords\n",
    "    for category, keywords in categories.items():\n",
    "        if any(keyword in article_content.lower() for keyword in keywords):\n",
    "            assigned_category = category\n",
    "            break\n",
    "    print(f\"Assigned category: {assigned_category}\")\n",
    "    return assigned_category\n",
    "\n",
    "# Example usage with an article\n",
    "#article_content = \"\"\"The government has announced a new policy that will affect the election process.\"\"\"\n",
    "#category = determine_category(article_content)\n",
    "#print(f\"Assigned category: {category}\")  # Should print \"Politics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06b8379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_mongo(articles):\n",
    "    \"\"\"Save articles to MongoDB, avoiding duplicates based on title similarity across sources.\"\"\"\n",
    "    # Fetch all existing articles from the database for comparison\n",
    "    existing_articles = list(collection.find({}, {'title': 1}))  # Only fetch titles for comparison\n",
    "\n",
    "    for article in articles:\n",
    "        # Check for duplicate articles based on title similarity\n",
    "        if not is_duplicate_article(article, existing_articles):\n",
    "            collection.insert_one(article)\n",
    "            print(f\"Saved article: {article['title']}\")\n",
    "        else:\n",
    "            print(f\"Duplicate article skipped: {article['title']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78649a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rss_feed(rss_url, driver, portal):\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    title_or = \"\"\n",
    "    description_or = \"\"\n",
    "    title = \"\"\n",
    "    description = \"\"\n",
    "    news_data = []\n",
    "    images = []\n",
    "    \n",
    "    # Fetch existing titles from MongoDB for duplicate checking\n",
    "    existing_titles = list(collection.find({}, {'title': 1}))\n",
    "    #print(f\"Skipping duplicate titles: {existing_titles}\")\n",
    "    existing_titles = [item['title'] for item in existing_titles]\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        title_or = entry.title\n",
    "        \n",
    "        url = entry.link\n",
    "        article_details = ''\n",
    "\n",
    "        # Check for duplicates\n",
    "        if is_duplicate_article({\"title\": title}, existing_titles):\n",
    "            print(f\"Duplicate article skipped: {title}\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        print(f\"Portal: {portal}\")\n",
    "\n",
    "        # Fetch additional details from the article page\n",
    "        article_details = fetch_full_article_bbc(url, driver)\n",
    "        \n",
    "        if article_details.get(\"description\", \"\") != \"\":\n",
    "\n",
    "            description_or = article_details.get(\"description\", \"\")\n",
    "\n",
    "            \n",
    "            category = determine_category(description_or)\n",
    "\n",
    "            print(f\"Title: {title}\")  # Should print the Title\n",
    "            images = article_details.get(\"images\", [])\n",
    "\n",
    "            \n",
    "            \n",
    "            title = clean_sensationalism(title_or)\n",
    "            description = clean_sensationalism(description_or)\n",
    "            \n",
    "        \n",
    "            # Use .get() to avoid KeyError for missing fields\n",
    "            news_item = {\n",
    "                \"title_or\": title_or,\n",
    "                \"description_or\": description_or,                \n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"url\": url,\n",
    "                \"published_time\": article_details.get(\"published_time\", \"\"),\n",
    "                \"author_name\": article_details.get(\"author_name\", \"\"),\n",
    "                \"author_designation\": article_details.get(\"author_designation\", \"\"),  # Optional field\n",
    "                \"reporting_location\": article_details.get(\"reporting_location\", \"\"),  # Optional field\n",
    "                \"images\": images[0],  # Optional field, default to empty list\n",
    "                \"source\": portal,\n",
    "                \"category\": category,\n",
    "                \"like\": 0,\n",
    "                \"comment\": 0,\n",
    "                \"share\": 0,\n",
    "                \"follow\": 0,\n",
    "                \"left\": 0,\n",
    "                \"center\": 0,\n",
    "                \"right\": 0,\n",
    "                \"fetched_at\": datetime.now()\n",
    "            }\n",
    "            #print(news_item)  # Print news details to the console\n",
    "            news_data.append(news_item)\n",
    "    \n",
    "    return news_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "523e5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news():\n",
    "    \"\"\"Crawl all news outlets and save to MongoDB\"\"\"    \n",
    "    driver = setup_driver()\n",
    "    rss_url = \"http://feeds.bbci.co.uk/news/rss.xml\"\n",
    "    print(rss_url);\n",
    "    try:\n",
    "        # Handle other portals with single URL feeds\n",
    "        news_data = parse_rss_feed(rss_url, driver, 'bbc')\n",
    "        save_articles_to_mongo(news_data)\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c72c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://feeds.bbci.co.uk/news/rss.xml\n",
      "Portal: bbc\n",
      "Time: 4 hours ago\n",
      "Assigned category: World\n",
      "Title: \n",
      "Sentences: ['Calls for Archbishop of York to resign over Church failings in sex abuse case']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_analyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     crawl_news()\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m, in \u001b[0;36mcrawl_news\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(rss_url);\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Handle other portals with single URL feeds\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     news_data \u001b[38;5;241m=\u001b[39m parse_rss_feed(rss_url, driver, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m     save_articles_to_mongo(news_data)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[20], line 44\u001b[0m, in \u001b[0;36mparse_rss_feed\u001b[0;34m(rss_url, driver, portal)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should print the Title\u001b[39;00m\n\u001b[1;32m     40\u001b[0m images \u001b[38;5;241m=\u001b[39m article_details\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m---> 44\u001b[0m title \u001b[38;5;241m=\u001b[39m clean_sensationalism(title_or)\n\u001b[1;32m     45\u001b[0m description \u001b[38;5;241m=\u001b[39m clean_sensationalism(description_or)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Use .get() to avoid KeyError for missing fields\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mclean_sensationalism\u001b[0;34m(article_text)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Analyze each sentence\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m---> 22\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m sentiment_analyzer(sentence)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentiment[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEGATIVE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOSITIVE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m sentiment[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# Skip overly emotional sentences or rewrite them\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping sensational sentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentiment_analyzer' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    crawl_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    " # Fetch all articles from the collection\n",
    "driver = setup_driver()\n",
    "articles = list(collection.find())\n",
    "print(driver)\n",
    "for article in articles:\n",
    "    if article['source'] == \"bbc\" and article['description'] == \"Full content not available\" :\n",
    "        print(article['url'])\n",
    "        news_data = parse_rss_feed(article['url'], driver, 'bbc')\n",
    "        print(news_data);\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
