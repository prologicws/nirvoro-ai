{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954952b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: feedparser in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (6.0.11)\n",
      "Requirement already satisfied: pymongo in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.10.1)\n",
      "Requirement already satisfied: rapidfuzz in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: selenium in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.25.0)\n",
      "Requirement already satisfied: transformers in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: torch in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: sgmllib3k in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: filelock in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: sympy in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from urllib3<3,>=1.21.1->requests) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 feedparser pymongo rapidfuzz selenium transformers torch\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from rapidfuzz import fuzz\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from transformers import pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ec0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of RSS feed URLs\n",
    "rss_feeds = {\n",
    "    \"BBC\": \"http://feeds.bbci.co.uk/news/rss.xml\",\n",
    "    \"CNN\": \"http://rss.cnn.com/rss/edition.rss\",\n",
    "    \"Reuters\": [\n",
    "        \"https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=business-finance&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=deals&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=political-general&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=environment&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=tech&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=health&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=sports&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=lifestyle-entertainment&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=human-interest&post_type=best\",\n",
    "        \"https://www.reutersagency.com/feed/?best-topics=journalist-spotlight&post_type=best\",\n",
    "    ],\n",
    "    \n",
    "    \"TOI\": \"https://timesofindia.indiatimes.com/rssfeedstopstories.cms\", #The New York Times\n",
    "    #\"TNYT\": \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\", #The New York Times\n",
    "    #\"TG\": \"https://www.theguardian.com/uk/rss\", #The Guardian\n",
    "    #\"AJ\": \"https://www.aljazeera.com/xml/rss/all.xml\" #Al Jazeera\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386da59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Setup\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Update with your MongoDB connection URI\n",
    "db = client['news_database']\n",
    "collection = db['news_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fa28918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Load a summarization model and sentiment analysis model\n",
    "summarizer = pipeline('summarization')\n",
    "sentiment_analyzer = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2478417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # Runs Chrome in headless mode\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service('/opt/homebrew/bin/chromedriver')  # Update this path if necessary\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a77062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relative_time_to_iso(relative_time):\n",
    "    \"\"\"\n",
    "    Convert a relative time string like '21 hours ago' to an ISO timestamp.\n",
    "    \"\"\"\n",
    "    # Get the current time\n",
    "    current_time = datetime.utcnow()\n",
    "\n",
    "    # Use regular expressions to extract time units and values\n",
    "    match = re.match(r\"(\\d+)\\s*(\\w+)\\s*ago\", relative_time)\n",
    "    if not match:\n",
    "        return None  # Return None if the format is not recognized\n",
    "\n",
    "    amount, unit = int(match.group(1)), match.group(2)\n",
    "\n",
    "    # Convert the time unit into a timedelta object\n",
    "    if \"hour\" in unit:\n",
    "        time_delta = timedelta(hours=amount)\n",
    "    elif \"minute\" in unit:\n",
    "        time_delta = timedelta(minutes=amount)\n",
    "    elif \"second\" in unit:\n",
    "        time_delta = timedelta(seconds=amount)\n",
    "    elif \"day\" in unit:\n",
    "        time_delta = timedelta(days=amount)\n",
    "    else:\n",
    "        return None  # If it's a unit we don't handle, return None\n",
    "\n",
    "    # Subtract the timedelta from the current time\n",
    "    calculated_time = current_time - time_delta\n",
    "\n",
    "    # Convert to ISO format\n",
    "    return calculated_time.isoformat() + \"Z\"  # Adding 'Z' to denote UTC time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba29a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cnn_time(cnn_time):\n",
    "    # Step 1: Remove \"Updated\" text\n",
    "    cnn_time = cnn_time.replace(\"Updated\", \"\").strip()\n",
    "\n",
    "    # Step 2: Parse the time (assuming it's in Eastern Daylight Time - EDT)\n",
    "    # We'll remove the EDT part and parse the rest of the time\n",
    "    time_format = \"%I:%M %p %Z, %a %B %d, %Y\"\n",
    "    cnn_time = cnn_time.replace(\"EDT\", \"Eastern\")  # Handle EDT as \"Eastern\" for parsing\n",
    "    dt = datetime.strptime(cnn_time, time_format)\n",
    "    \n",
    "    # Convert to UTC or desired timezone (for example, GMT)\n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    dt_eastern = eastern.localize(dt)  # Localize the datetime to Eastern Time\n",
    "    dt_utc = dt_eastern.astimezone(pytz.utc)  # Convert to UTC\n",
    "\n",
    "    # Step 3: Convert to desired format (similar to BBC format)\n",
    "    return dt_utc.strftime('%d %b %Y, %H:%M GMT')\n",
    "\n",
    "# Test with your input example\n",
    "#cnn_published_time = \"Updated  4:09 PM EDT, Thu April 6, 2023\"\n",
    "#formatted_time = convert_cnn_time(cnn_published_time)\n",
    "#print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb32a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch full news content and additional details from the news article page\n",
    "def fetch_full_article_bbc(url, driver):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        #time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Initialize fields\n",
    "        full_text = \"\"\n",
    "        published_time = \"\"\n",
    "        published_time_raw = \"\"\n",
    "        author_name = \"\"\n",
    "        author_designation = \"\"\n",
    "        reporting_location = \"\"\n",
    "        images = []\n",
    "\n",
    "        # Extract the article content\n",
    "        article = soup.find('article')\n",
    "        if article:\n",
    "            # Extract paragraphs\n",
    "            paragraphs = article.find_all('p')\n",
    "            full_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "            #print(f\"Full text: {full_text}\")\n",
    "\n",
    "            # Extract published time\n",
    "            time_tag = article.find('time')\n",
    "            if time_tag:\n",
    "                published_time_raw = time_tag.get_text()                \n",
    "                print(f\"Time: {published_time_raw}\")\n",
    "                published_time = convert_relative_time_to_iso(published_time_raw)\n",
    "\n",
    "\n",
    "            # Extract author name and designation\n",
    "            byline_block = article.find('div', {'data-component': 'byline-block'})\n",
    "            if byline_block:\n",
    "                author_name_tag = byline_block.find('span', class_='bZCrck')\n",
    "                if author_name_tag:\n",
    "                    author_name = author_name_tag.get_text()\n",
    "\n",
    "                author_designation_tag = byline_block.find('div', class_='hEbjLr')\n",
    "                if author_designation_tag:\n",
    "                    author_designation = author_designation_tag.get_text()\n",
    "\n",
    "                # Extract reporting location\n",
    "                reporting_location_tag = byline_block.find('span', string=lambda x: x and \"Reporting from\" in x)\n",
    "                if reporting_location_tag:\n",
    "                    reporting_location = reporting_location_tag.get_text().replace(\"Reporting from\", \"\").strip()\n",
    "        \n",
    "        # Extract images within <figure> tags\n",
    "        figures = soup.find_all('figure')\n",
    "        for fig in figures:\n",
    "            img = fig.find('img')\n",
    "            if img and img.get('src'):\n",
    "                # Some images might have relative URLs\n",
    "                img_url = img['src']\n",
    "                if img_url.startswith('//'):\n",
    "                    img_url = 'https:' + img_url\n",
    "                elif img_url.startswith('/'):\n",
    "                    img_url = 'https://www.bbc.com' + img_url\n",
    "                images.append(img_url)\n",
    "        \n",
    "        return {\n",
    "            \"description\": full_text if full_text else \"Full content not available\",\n",
    "            \"published_time\": published_time,\n",
    "            \"author_name\": author_name,\n",
    "            \"author_designation\": author_designation,\n",
    "            \"reporting_location\": reporting_location,\n",
    "            \"images\": images\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch full content from {url}: {e}\")\n",
    "        return {\n",
    "            \"description\": \"Failed to fetch full content\",\n",
    "            \"published_time\": \"\",\n",
    "            \"author_name\": \"\",\n",
    "            \"author_designation\": \"\",\n",
    "            \"reporting_location\": \"\",\n",
    "            \"images\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab7ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch CNN article data\n",
    "def fetch_full_article_cnn(url, driver):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        #response = requests.get(url)\n",
    "        #soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # 1. Extract the headline\n",
    "        headline_tag = soup.find('h1', {'class': 'headline__text'})\n",
    "        headline = headline_tag.get_text(strip=True) if headline_tag else 'Headline not found'\n",
    "        print(f\"headline_tag: {headline_tag}\")\n",
    "        print(f\"headline: {headline}\")\n",
    "\n",
    "        # 2. Extract the author name\n",
    "        author_tag = soup.find('div', {'class': 'byline__names'})\n",
    "        author_name = author_tag.find('span', {'class': 'byline__name'}).get_text(strip=True) if author_tag else 'Author not found'\n",
    "        print(f\"author_tag: {author_tag}\")\n",
    "        print(f\"author_name: {author_name}\")\n",
    "        \n",
    "        # 3. Extract the published time\n",
    "        timestamp_tag = soup.find('div', {'class': 'timestamp'})\n",
    "        published_time_raw = timestamp_tag.get_text(strip=True) if timestamp_tag else 'Published time not found'        \n",
    "        formatted_time = convert_cnn_time(published_time_raw)\n",
    "        \n",
    "        print(f\"timestamp_tag: {timestamp_tag}\")\n",
    "        print(f\"published_time: {published_time}\")\n",
    "\n",
    "        # 4. Extract images (multiple)\n",
    "        image_tags = soup.find_all('div', {'class': 'image__lede article__lede-wrapper'})\n",
    "        images = []\n",
    "        for img_tag in image_tags:\n",
    "            img = img_tag.find('img')\n",
    "            if img and img.get('src'):\n",
    "                images.append(img['src'])\n",
    "\n",
    "        # 5. Extract description/content\n",
    "        description_tag = soup.find('div', {'class': 'article__content-container'})\n",
    "        description = description_tag.get_text(strip=True) if description_tag else 'Description not found'\n",
    "\n",
    "        # Return the extracted data as a dictionary\n",
    "        return {\n",
    "            'headline': headline,\n",
    "            'author_name': author_name,\n",
    "            'published_time': published_time,\n",
    "            'images': images,\n",
    "            'description': description\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch full content from {url}: {e}\")\n",
    "        return {\n",
    "            \"description\": \"Failed to fetch full content\",\n",
    "            \"published_time\": \"\",\n",
    "            \"author_name\": \"\",\n",
    "            \"author_designation\": \"\",\n",
    "            \"reporting_location\": \"\",\n",
    "            \"images\": []\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8696a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_times_of_india_news():\n",
    "    base_url = \"https://timesofindia.indiatimes.com/india\"\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    news_snippets = soup.find_all('div', {'class': 'iN5CR'})\n",
    "\n",
    "    # Loop through each snippet to extract individual news links\n",
    "    for snippet in news_snippets:\n",
    "        link_tag = snippet.find('a', href=True)\n",
    "        if link_tag:\n",
    "            article_url = link_tag['href']\n",
    "            print(f\"Fetching article from: {article_url}\")\n",
    "\n",
    "            # Fetch full article details\n",
    "            article_data = fetch_full_article_times_of_india(article_url)\n",
    "            if article_data:\n",
    "                save_articles_to_mongo(article_data)\n",
    "\n",
    "            time.sleep(1)  # Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c793a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the full Reuters article data\n",
    "def fetch_full_article_reuters(rss_article_url, driver):\n",
    "    try:\n",
    "        # Step 1: Navigate to the RSS article page\n",
    "        # Simulate mouse movement\n",
    "        action = ActionChains(driver)\n",
    "        element = driver.find_element_by_xpath(\"//button[@class='submit']\")\n",
    "        action.move_to_element(element).perform()\n",
    "\n",
    "        # Randomize delay\n",
    "        time.sleep(2 + random.random() * 3)  # Pause 2 to 5 seconds\n",
    "\n",
    "        # Click the button to trigger a click event manually\n",
    "        element.click()\n",
    "\n",
    "        driver.get(rss_article_url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Step 2: Find the actual article link from the RSS article page\n",
    "        article_link_tag = soup.find('a', href=True, text=\"exclusively reported\")\n",
    "        \n",
    "        if not article_link_tag:\n",
    "            print(f\"Actual article link not found on {rss_article_url}\")\n",
    "            return {\"description\": \"Failed to fetch full content\", \"author_name\": \"\", \"category\": \"\", \"headline\": \"Headline not found\"}\n",
    "\n",
    "        article_url = article_link_tag['href']\n",
    "        print(f\"Found article link: {article_url}\")\n",
    "        \n",
    "        # Step 3: Navigate to the actual article page\n",
    "        driver.get(article_url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Step 4: Extract title\n",
    "        title_tag = soup.find('h1', {'data-testid': 'Heading'})\n",
    "        headline = title_tag.get_text(strip=True) if title_tag else 'Headline not found'\n",
    "        print(f\"Title: {headline}\")\n",
    "        \n",
    "        # Step 5: Extract author\n",
    "        author_tag = soup.find('a', rel='author')\n",
    "        author_name = author_tag.get_text(strip=True) if author_tag else 'Author not found'\n",
    "        print(f\"Author: {author_name}\")\n",
    "        \n",
    "        # Step 6: Extract category\n",
    "        category_tag = soup.find('ul', {'class': 'tags-with-tooltip__list__37vkr'})\n",
    "        category = category_tag.get_text(strip=True) if category_tag else 'Category not found'\n",
    "        print(f\"Category: {category}\")\n",
    "        \n",
    "        # Step 7: Extract description/content\n",
    "        description_tag = soup.find('div', {'class': 'article-body__content__17Yit'})\n",
    "        description = description_tag.get_text(strip=True) if description_tag else 'Description not found'\n",
    "        print(f\"Description: {description}\")\n",
    "\n",
    "        # Return the extracted data as a dictionary\n",
    "        return {\n",
    "            'headline': headline,\n",
    "            'author_name': author_name,\n",
    "            'category': category,\n",
    "            'description': description,\n",
    "            'published_time': datetime.now().isoformat(),  # You can replace this with actual fetched timestamp logic\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch full content from {rss_article_url}: {e}\")\n",
    "        return {\n",
    "            \"description\": \"Failed to fetch full content\",\n",
    "            \"author_name\": \"\",\n",
    "            \"category\": \"\",\n",
    "            \"headline\": \"Headline not found\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b246ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_article_times_of_india(url):\n",
    "    \"\"\"\n",
    "    Fetches the full article details from a Times of India news page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title_tag = soup.find('h1', {'class': 'HNMDR'})\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'Title not found'\n",
    "\n",
    "        summary_tag = soup.find('div', {'class': 'M1rHh'})\n",
    "        summary = summary_tag.get_text(strip=True) if summary_tag else 'Summary not found'\n",
    "\n",
    "        byline_tag = soup.find('div', {'class': 'xf8Pm byline'})\n",
    "        if byline_tag:\n",
    "            byline_text = byline_tag.get_text(strip=True)\n",
    "            parts = byline_text.split('|')\n",
    "            author = parts[0].strip() if len(parts) > 0 else \"Author not found\"\n",
    "            published_time = parts[1].strip() if len(parts) > 1 else \"Date not found\"\n",
    "        else:\n",
    "            author = \"Author not found\"\n",
    "            published_time = \"Date not found\"\n",
    "\n",
    "        try:\n",
    "            published_time = datetime.strptime(published_time, '%b %d, %Y, %I:%M %p').isoformat()\n",
    "        except ValueError:\n",
    "            pass  # Keep original string if parsing fails\n",
    "\n",
    "        content_tag = soup.find('div', {'data-articlebody': '1'})\n",
    "        content = content_tag.get_text(strip=True) if content_tag else 'Content not found'\n",
    "\n",
    "        return {\n",
    "            'title': title,\n",
    "            'summary': summary,\n",
    "            'author': author,\n",
    "            'published_time': published_time,\n",
    "            'content': content,\n",
    "            'url': url,\n",
    "            'source': 'Times of India'  # Adding source field\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "475d3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_tag(entry):\n",
    "    \"\"\"Assign a specific tag to an article based on custom logic\"\"\"\n",
    "    if 'breaking' in entry.title.lower():\n",
    "        return 1  # Tag for breaking news\n",
    "    elif 'politics' in entry.title.lower():\n",
    "        return 2  # Tag for COVID-related news\n",
    "    elif 'business' in entry.title.lower():\n",
    "        return 3  # Tag for election news\n",
    "    elif 'science' in entry.title.lower():\n",
    "        return 4  # Tag for election news\n",
    "    elif 'sports' in entry.title.lower():\n",
    "        return 5  # Tag for election news\n",
    "    elif 'arts' in entry.title.lower():\n",
    "        return 6  # Tag for election news\n",
    "    elif 'entertainment' in entry.title.lower():\n",
    "        return 7  # Tag for election news\n",
    "    elif 'travel' in entry.title.lower():\n",
    "        return 8  # Tag for election news\n",
    "    elif 'weather' in entry.title.lower():\n",
    "        return 9  # Tag for election news\n",
    "    elif 'earth' in entry.title.lower():\n",
    "        return 10  # Tag for election news\n",
    "    elif 'local' in entry.title.lower():\n",
    "        return 11  # Tag for election news\n",
    "    else:\n",
    "        return 0  # Default tag for other news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5de45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_article_other(new_article, existing_articles):\n",
    "    \"\"\"Check if the new article is a duplicate based on title similarity using rapidfuzz.\"\"\"\n",
    "    \n",
    "    for article_item in existing_articles:\n",
    "        if 'headline' in article_item:\n",
    "            similarity = fuzz.token_set_ratio(new_article['headline'], article_item['headline'])\n",
    "            if similarity > 85:  # Threshold for considering articles as duplicates\n",
    "                print(f\"Duplicate title found: {article_item['headline']}\")\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d951ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_article(new_article, existing_articles):\n",
    "    \"\"\"\n",
    "    Check if the new article is a duplicate based on title similarity and source.\n",
    "    If the title matches exactly and the source is the same, it's considered a duplicate.\n",
    "    \"\"\"\n",
    "    for article_item in existing_articles:\n",
    "        # Ensure the existing article has a headline and source\n",
    "        if 'headline' in article_item and 'source' in article_item:\n",
    "            if article_item['headline'] == new_article['headline'] and article_item['source'] == new_article['source']:\n",
    "                print(f\"Duplicate title found in the same source: {article_item['headline']} (Source: {article_item['source']})\")\n",
    "                return True\n",
    "        else:\n",
    "            # If either headline or source is missing, we can't reliably check for duplicates\n",
    "            continue\n",
    "            \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b7b05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_and_save_duplicate_articles():\n",
    "    \"\"\"Mark duplicate articles with the same tagId based on title similarity and save in the DB.\"\"\"\n",
    "    \n",
    "    # Fetch all articles from the collection\n",
    "    articles = list(collection.find())\n",
    "    \n",
    "    current_tag_id = 1  # Starting tagId\n",
    "\n",
    "    # Track tagIds for already tagged groups of duplicates\n",
    "    assigned_tag_ids = {}\n",
    "\n",
    "    for i, new_article in enumerate(articles):\n",
    "        if 'tagId' in new_article and new_article['tagId'] is not None:\n",
    "            continue  # Skip already tagged articles\n",
    "\n",
    "        # If the article is not tagged, assign a new tagId\n",
    "        new_article_tag_id = current_tag_id\n",
    "\n",
    "        # Update the current article with the tagId\n",
    "        new_article['_id']  # Get the ID of the new article\n",
    "        collection.update_one({'_id': new_article['_id']}, {'$set': {'tagId': new_article_tag_id}})\n",
    "\n",
    "        # Compare the current article with the rest to find duplicates\n",
    "        for j in range(i + 1, len(articles)):\n",
    "            existing_article = articles[j]\n",
    "\n",
    "            if 'headline' in existing_article:\n",
    "                similarity = fuzz.token_set_ratio(new_article['headline'], existing_article['headline'])\n",
    "\n",
    "                if similarity > 85:\n",
    "                    print(f\"Duplicate title found: {existing_article['headline']}\")\n",
    "\n",
    "                    # Check if the existing article already has a tagId\n",
    "                    if 'tagId' in existing_article and existing_article['tagId'] is not None:\n",
    "                        # Use the existing article's tagId for the current article group\n",
    "                        new_article_tag_id = existing_article['tagId']\n",
    "                    else:\n",
    "                        # Assign the current tagId to the existing duplicate article\n",
    "                        existing_article['tagId'] = new_article_tag_id\n",
    "                        collection.update_one({'_id': existing_article['_id']}, {'$set': {'tagId': new_article_tag_id}})\n",
    "\n",
    "        # Increment the tagId only if it was a unique article (no duplicates found)\n",
    "        if new_article_tag_id == current_tag_id:\n",
    "            current_tag_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06b8379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_mongo(articles):\n",
    "    \"\"\"Save articles to MongoDB, avoiding duplicates based on title similarity across sources.\"\"\"\n",
    "    # Fetch all existing articles from the database for comparison\n",
    "    existing_articles = list(collection.find({}, {'headline': 1}))  # Only fetch titles for comparison\n",
    "\n",
    "    for article in articles:\n",
    "        # Check for duplicate articles based on title similarity\n",
    "        if not is_duplicate_article(article, existing_articles):\n",
    "            collection.insert_one(article)\n",
    "            print(f\"Saved article: {article['headline']}\")\n",
    "        else:\n",
    "            print(f\"Duplicate article skipped: {article['headline']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2934393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_summarize_text(text, max_len=50, min_len=20):\n",
    "    \"\"\"\n",
    "    Clean sensationalism and summarize the input text (title or description).\n",
    "    \"\"\"\n",
    "    # Summarize the input text (if needed)\n",
    "    summary = summarizer(text, max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Bolden important points in the summary\n",
    "    cleaned_summary = bolden_important_points(summary)\n",
    "    \n",
    "    return cleaned_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f8f27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bolden_important_points(text):\n",
    "    \"\"\"\n",
    "    Bolden important points in the article text (e.g., first sentence, named entities).\n",
    "    \"\"\"\n",
    "    # Split the summary into sentences\n",
    "    sentences = text.split('. ')\n",
    "    \n",
    "    # Bolden first sentence or key information\n",
    "    if sentences:\n",
    "        sentences[0] = f\"<b>{sentences[0]}</b>\"\n",
    "    \n",
    "    # Join sentences back into a single string\n",
    "    boldened_text = '. '.join(sentences)\n",
    "    \n",
    "    return boldened_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d118074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all articles from the collection\n",
    "#articles = []\n",
    "#articles = list(collection.find())\n",
    "#mark_and_save_duplicate_articles();\n",
    "\n",
    "def clean_sensationalism(article_text):\n",
    "    \"\"\"\n",
    "    Cleans sensationalism by detecting overly emotional or sensational language \n",
    "    and rephrasing the text to focus on the facts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Break the text into sentences\n",
    "    if not article_text:  # Check if article_text is None or empty\n",
    "        return \"Content not available or cannot be cleaned.\"\n",
    "    \n",
    "    sentences = article_text.split('. ')\n",
    "    factual_sentences = []\n",
    "    print(f\"Sentences: {sentences}\")\n",
    "\n",
    "    # Analyze each sentence\n",
    "    for sentence in sentences:\n",
    "        sentiment = sentiment_analyzer(sentence)\n",
    "        if sentiment[0]['label'] in ['NEGATIVE', 'POSITIVE'] and sentiment[0]['score'] > 0.7:\n",
    "            # Skip overly emotional sentences or rewrite them\n",
    "            print(f\"Skipping sensational sentence: {sentence}\")\n",
    "            continue\n",
    "        factual_sentences.append(sentence)\n",
    "\n",
    "    # Join the factual sentences\n",
    "    clean_text = '. '.join(factual_sentences)\n",
    "    \n",
    "    # Use summarization to condense the cleaned text\n",
    "    summary = summarizer(clean_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage with article data\n",
    "article_text = \"\"\"\n",
    "    This shocking event has left the whole world in disbelief! You won't believe what happened next. \n",
    "    After years of struggle, the company has finally decided to change its strategy. \n",
    "    Experts say that this could lead to a market crash of unprecedented proportions. \n",
    "    The fact is, the company announced its new plans in a statement released yesterday.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ad1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "# Clean the article of sensationalism and rephrase\n",
    "    if article.get('description') != 'Full content not available':\n",
    "        cleaned_headline = clean_sensationalism(article.get('headline'))\n",
    "        cleaned_description = clean_sensationalism(article.get('description'))\n",
    "        #print(\"Cleaned and concise article: \")\n",
    "        print(f\"{cleaned_headline}\\n\")        \n",
    "        print(f\"{cleaned_description}\\n\")\n",
    "        \n",
    "        # Update the document with the new fields\n",
    "        collection.update_one(\n",
    "            {'_id': article['_id']},\n",
    "            {'$set': {\n",
    "                'cleaned_summary': cleaned_description,\n",
    "                'cleaned_headline': cleaned_headline\n",
    "            }}\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "649c5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_category(article_content):\n",
    "    \"\"\"\n",
    "    Determines the category of the article based on its content.\n",
    "    \"\"\"\n",
    "    # Define keywords for each category\n",
    "    categories = {\n",
    "        \"Breaking\": [\"breaking\", \"urgent\", \"just in\"],\n",
    "        \"Politics\": [\"government\", \"policy\", \"election\", \"political\"],\n",
    "        \"Business\": [\"business\", \"market\", \"stocks\", \"finance\", \"economy\"],\n",
    "        \"Science\": [\"research\", \"scientists\", \"study\", \"laboratory\", \"experiment\"],\n",
    "        \"Sports\": [\"game\", \"match\", \"tournament\", \"league\", \"athlete\"],\n",
    "        \"Art & Culture\": [\"art\", \"museum\", \"culture\", \"festival\", \"heritage\"],\n",
    "        \"Entertainment\": [\"celebrity\", \"movie\", \"music\", \"show\", \"award\"],\n",
    "        \"Travel\": [\"travel\", \"tourism\", \"destination\", \"flight\", \"hotel\"],\n",
    "        \"Weather\": [\"weather\", \"storm\", \"temperature\", \"forecast\"],\n",
    "        \"Earth\": [\"climate\", \"environment\", \"sustainability\", \"nature\"],\n",
    "        \"Local\": [\"local\", \"community\", \"neighborhood\", \"town\"]\n",
    "    }\n",
    "\n",
    "    # Default category\n",
    "    assigned_category = \"Top Stories\"\n",
    "\n",
    "    # Check for category keywords\n",
    "    for category, keywords in categories.items():\n",
    "        if any(keyword in article_content.lower() for keyword in keywords):\n",
    "            assigned_category = category\n",
    "            break\n",
    "    print(f\"Assigned category: {assigned_category}\")\n",
    "    return assigned_category\n",
    "\n",
    "# Example usage with an article\n",
    "#article_content = \"\"\"The government has announced a new policy that will affect the election process.\"\"\"\n",
    "#category = determine_category(article_content)\n",
    "#print(f\"Assigned category: {category}\")  # Should print \"Politics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab245d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rss_feed(rss_url, driver, portal):\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    \n",
    "    news_data = []\n",
    "    # Fetch existing titles from MongoDB for duplicate checking\n",
    "    existing_titles = list(collection.find({}, {'headline': 1}))\n",
    "    #print(f\"Skipping duplicate titles: {existing_titles}\")\n",
    "    existing_titles = [item['headline'] for item in existing_titles]\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        headline = entry.title\n",
    "        url = entry.link\n",
    "        article_details = ''\n",
    "\n",
    "        # Check for duplicates\n",
    "        if is_duplicate_article({\"headline\": headline}, existing_titles):\n",
    "            print(f\"Duplicate article skipped: {headline}\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        print(f\"Portal: {portal}\")\n",
    "\n",
    "        # Fetch additional details from the article page\n",
    "        if portal == 'bbc':\n",
    "            article_details = fetch_full_article_bbc(url, driver)\n",
    "        elif portal == 'cnn':\n",
    "            article_details = fetch_full_article_cnn(url, driver)\n",
    "        elif portal == 'reuters':\n",
    "            article_details = fetch_full_article_reuters(url, driver)\n",
    "        elif portal == 'toi':\n",
    "            article_details = fetch_full_article_times_of_india(url, driver)\n",
    "            \n",
    "        if article_details.get(\"description\", \"\") != \"\":\n",
    "            category = determine_category(article_details.get(\"description\", \"\"))\n",
    "\n",
    "            print(f\"Headline: {headline}\")  # Should print the Headline\n",
    "            \n",
    "\n",
    "            # Use .get() to avoid KeyError for missing fields\n",
    "            news_item = {\n",
    "                \"headline\": headline,\n",
    "                \"url\": url,\n",
    "                \"description\": article_details.get(\"description\", \"\"),\n",
    "                \"published_time\": article_details.get(\"published_time\", \"\"),\n",
    "                \"author_name\": article_details.get(\"author_name\", \"\"),\n",
    "                \"author_designation\": article_details.get(\"author_designation\", \"\"),  # Optional field\n",
    "                \"reporting_location\": article_details.get(\"reporting_location\", \"\"),  # Optional field\n",
    "                \"images\": article_details.get(\"images\", []),  # Optional field, default to empty list\n",
    "                \"source\": portal,\n",
    "                \"category\": category,\n",
    "                \"fetched_at\": datetime.now()\n",
    "            }\n",
    "            #print(news_item)  # Print news details to the console\n",
    "            news_data.append(news_item)\n",
    "    \n",
    "    return news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6746954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news():\n",
    "    \"\"\"Crawl all news outlets and save to MongoDB\"\"\"    \n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        # Loop through RSS feeds\n",
    "        for portal, rss_url in rss_feeds.items():\n",
    "            #print(f\"Fetching RSS URL {rss_url}\")\n",
    "            \n",
    "            # Check if the portal is \"Reuters\"\n",
    "            if portal == \"Reuters\":\n",
    "                # rss_url is a list, so loop through it directly\n",
    "                for rss_sub_url in rss_url:\n",
    "                    print(f\"Fetching news from {rss_sub_url}\")\n",
    "                    # Call your parsing function for each sub-URL\n",
    "                    news_data = parse_rss_feed(rss_sub_url, driver, portal.lower().replace(' ', '_'))\n",
    "                    save_articles_to_mongo(news_data)\n",
    "            else:\n",
    "                # Handle other portals with single URL feeds\n",
    "                news_data = parse_rss_feed(rss_url, driver, portal.lower().replace(' ', '_'))\n",
    "                save_articles_to_mongo(news_data)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "articles = fetch_times_of_india_news()\n",
    "for article in articles:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portal: bbc\n",
      "Time: 3 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 8 hours ago\n",
      "Assigned category: Art & Culture\n",
      "Assigned category: Art & Culture\n",
      "Portal: bbc\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: bbc\n",
      "Time: 1 hour ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 4 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 2 hours ago\n",
      "Assigned category: Breaking\n",
      "Assigned category: Breaking\n",
      "Portal: bbc\n",
      "Time: 2 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 14 hours ago\n",
      "Assigned category: Business\n",
      "Assigned category: Business\n",
      "Portal: bbc\n",
      "Time: 14 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: bbc\n",
      "Time: 1 day ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 7 hours ago\n",
      "Assigned category: Art & Culture\n",
      "Assigned category: Art & Culture\n",
      "Portal: bbc\n",
      "Time: 2 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 57 minutes ago\n",
      "Assigned category: Art & Culture\n",
      "Assigned category: Art & Culture\n",
      "Portal: bbc\n",
      "Time: 4 hours ago\n",
      "Assigned category: Breaking\n",
      "Assigned category: Breaking\n",
      "Portal: bbc\n",
      "Time: 20 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 15 hours ago\n",
      "Assigned category: Breaking\n",
      "Assigned category: Breaking\n",
      "Portal: bbc\n",
      "Time: 17 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 14 hours ago\n",
      "Assigned category: Breaking\n",
      "Assigned category: Breaking\n",
      "Portal: bbc\n",
      "Time: 21 hours ago\n",
      "Assigned category: Sports\n",
      "Assigned category: Sports\n",
      "Portal: bbc\n",
      "Time: 15 hours ago\n",
      "Assigned category: Art & Culture\n",
      "Assigned category: Art & Culture\n",
      "Portal: bbc\n",
      "Time: 7 hours ago\n",
      "Assigned category: Science\n",
      "Assigned category: Science\n",
      "Portal: bbc\n",
      "Time: 14 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 2 hours ago\n",
      "Assigned category: Travel\n",
      "Assigned category: Travel\n",
      "Portal: bbc\n",
      "Time: 44 minutes ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 15 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: bbc\n",
      "Time: 7 hours ago\n",
      "Assigned category: Politics\n",
      "Assigned category: Politics\n",
      "Portal: bbc\n",
      "Time: 20 hours ago\n",
      "Assigned category: Sports\n",
      "Assigned category: Sports\n",
      "Portal: bbc\n",
      "Time: 20 August\n",
      "Assigned category: Breaking\n",
      "Assigned category: Breaking\n",
      "Portal: bbc\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: bbc\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: bbc\n",
      "Time: 3 hours ago\n",
      "Assigned category: Sports\n",
      "Assigned category: Sports\n",
      "Portal: bbc\n",
      "Time: 2 hours ago\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: bbc\n",
      "Time: 1 hour ago\n",
      "Assigned category: Sports\n",
      "Assigned category: Sports\n",
      "Portal: bbc\n",
      "Time: 7 hours ago\n",
      "Assigned category: Business\n",
      "Assigned category: Business\n",
      "Portal: bbc\n",
      "Time: 6 hours ago\n",
      "Assigned category: Art & Culture\n",
      "Assigned category: Art & Culture\n",
      "Portal: bbc\n",
      "Time: 1 hour ago\n",
      "Assigned category: Art & Culture\n",
      "Assigned category: Art & Culture\n",
      "Portal: bbc\n",
      "Time: 20 hours ago\n",
      "Assigned category: Entertainment\n",
      "Assigned category: Entertainment\n",
      "Saved article: What we know about Israel’s attack on Iran\n",
      "Saved article: Strikes on Iran suggest Israel may have heeded US warnings\n",
      "Saved article: Watch: Jeremy Bowen says Israel attack on Iran is 'another big escalation'\n",
      "Saved article: Why has Israel attacked Iran?\n",
      "Saved article: Time has come for reparations dialogue, Commonwealth heads agree\n",
      "Saved article: Chancellor set to hike employers' National Insurance to raise £20bn\n",
      "Saved article: Can Rachel Reeves use her defining Budget to escape UK's 'doom loop'?\n",
      "Saved article: 'We are in danger' - Spanish anti-tourism spills into winter season\n",
      "Saved article: 'It's my car - why should an under-21 not be allowed in it?'\n",
      "Saved article: Ros Atkins on… Would Donald Trump accept defeat?\n",
      "Saved article: What issues really matter to you? Let us know\n",
      "Saved article: On board the 18:31 to Aberystwyth as it smashed into another train\n",
      "Saved article: High-stakes vote decides Georgia's future path in Europe\n",
      "Saved article: BBC warned by Met Police over Tim Westwood report\n",
      "Saved article: Catfish killer who abused thousands brought down by phone calls and seized devices\n",
      "Saved article: Why the King can't say 'sorry' for slavery\n",
      "Saved article: The family shop saying goodbye after 64 years\n",
      "Saved article: How a deleted LinkedIn post was weaponised and seen by millions before the Southport riot\n",
      "Saved article: Cubans endure days without power as energy crisis hits hard\n",
      "Saved article: The track that caused so much chaos on the dancefloor it got banned\n",
      "Saved article: Mum's 'nightmare' after deaths of son and daughter just months apart\n",
      "Saved article: How a winning racehorse changed everything for girl with cancer\n",
      "Saved article: Firms must prevent sexual harassment under new law\n",
      "Saved article: Driver seriously hurt in fake police car robbery\n",
      "Saved article: Small boat Channel crossings pass total for 2023\n",
      "Saved article: Action demanded over surge in illegal meat imports\n",
      "Saved article: Watch: Beyoncé urges Americans to 'sing a new song' as she addresses Kamala Harris rally\n",
      "Saved article: Big UK emissions cut needed, says climate watchdog\n",
      "Saved article: Tommy Robinson remanded in custody ahead of court date\n",
      "Saved article: BBC News app\n",
      "Saved article: ​​Will Trump supporters accept the election result?\n",
      "Saved article: Canada, India, and their diplomatic death spiral\n",
      "Saved article: 'The best in England' - Stokes backs batters after defeat\n",
      "Saved article: England suffer defeat by Germany in seven-goal thriller\n",
      "Saved article: 'I want to be first full-back to win Ballon d'Or'\n",
      "Saved article: Big crowd turns out to attend a rave at a chippy\n",
      "Saved article: Marti Pellow: 'I can't get out of a building without singing that song'\n",
      "Saved article: Two brothers die in Ballymoney crash\n",
      "Saved article: Police officer jailed for breaking wife's back\n",
      "Portal: cnn\n",
      "headline_tag: None\n",
      "headline: Headline not found\n",
      "author_tag: <div class=\"byline__names\">\n",
      "\t\t\tBy <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/aditi-sandal\"><span class=\"byline__name\">Aditi Sangal</span></a>, <span class=\"byline__name\">Mike Hayes</span>, <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/maureen-chowdhury\"><span class=\"byline__name\">Maureen Chowdhury</span></a>, <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/amir-vera\"><span class=\"byline__name\">Amir Vera</span></a> and <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/shania-shelton\"><span class=\"byline__name\">Shania Shelton</span></a>, CNN\n",
      "\t\t</div>\n",
      "author_name: Aditi Sangal\n",
      "Failed to fetch full content from https://edition.cnn.com/webview/politics/live-news/trump-indictment-stormy-daniels-news-04-03-23/index.html: time data '2:12 PM Eastern, Tue April 4, 2023' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: <h1 class=\"headline__text inline-placeholder vossi-headline-text\" data-editable=\"headlineText\" id=\"maincontent\">\n",
      "      Haberman reveals why Trump attacked judge and his family in speech\n",
      "    </h1>\n",
      "headline: Haberman reveals why Trump attacked judge and his family in speech\n",
      "author_tag: None\n",
      "author_name: Author not found\n",
      "Failed to fetch full content from https://www.cnn.com/videos/politics/2023/04/05/maggie-haberman-donald-trump-speech-indictment-reaction-sot-cnntm-vpx.cnn: time data 'Published time not found' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: None\n",
      "headline: Headline not found\n",
      "author_tag: None\n",
      "author_name: Author not found\n",
      "Failed to fetch full content from https://www.cnn.com/collections/intl-trump-040223/: time data 'Published time not found' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: <h1 class=\"headline__text inline-placeholder vossi-headline-text\" data-editable=\"headlineText\" id=\"maincontent\">\n",
      "      READ: Trump indictment and statement of facts related to hush money payment\n",
      "    </h1>\n",
      "headline: READ: Trump indictment and statement of facts related to hush money payment\n",
      "Failed to fetch full content from https://cnn.it/411KYN7: 'NoneType' object has no attribute 'get_text'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline_tag: None\n",
      "headline: Headline not found\n",
      "author_tag: <div class=\"byline__names\">\n",
      "\t\t\tBy <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/tara-subramaniam\"><span class=\"byline__name\">Tara Subramaniam</span></a>, <span class=\"byline__name\">Jack Guy</span>, <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/aditi-sandal\"><span class=\"byline__name\">Aditi Sangal</span></a>, <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/maureen-chowdhury\"><span class=\"byline__name\">Maureen Chowdhury</span></a> and <span class=\"byline__name\">Mike Hayes</span>, CNN\n",
      "\t\t</div>\n",
      "author_name: Tara Subramaniam\n",
      "Failed to fetch full content from https://edition.cnn.com/webview/europe/live-news/russia-ukraine-war-news-04-03-23/index.html: time data '10:00 PM Eastern, Mon April 3, 2023' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: <h1 class=\"headline__text inline-placeholder vossi-headline-text\" data-editable=\"headlineText\" id=\"maincontent\">\n",
      "      Video shows moment of deadly explosion at cafe in Russia\n",
      "    </h1>\n",
      "headline: Video shows moment of deadly explosion at cafe in Russia\n",
      "author_tag: None\n",
      "author_name: Author not found\n",
      "Failed to fetch full content from https://www.cnn.com/videos/world/2023/04/02/russian-military-blogger-vladlen-tatarsky-killed-cafe-explosion-chance-vpx.cnn: time data 'Published time not found' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: None\n",
      "headline: Headline not found\n",
      "author_tag: None\n",
      "author_name: Author not found\n",
      "Failed to fetch full content from https://www.cnn.com/collections/intl-ukraine-030423/: time data 'Published time not found' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: <h1 class=\"headline__text inline-placeholder vossi-headline-text\" data-editable=\"headlineText\" id=\"maincontent\">\n",
      "      Chinese spy balloon was able to transmit information back to Beijing\n",
      "    </h1>\n",
      "headline: Chinese spy balloon was able to transmit information back to Beijing\n",
      "author_tag: <div class=\"byline__names\">\n",
      "\t\t\tBy <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/natasha-bertrand-profile\"><span class=\"byline__name\">Natasha Bertrand</span></a>, CNN\n",
      "\t\t</div>\n",
      "author_name: Natasha Bertrand\n",
      "Failed to fetch full content from https://www.cnn.com/2023/04/03/politics/chinese-spy-balloon/index.html: time data '9:38 AM Eastern, Mon April 3, 2023' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: <h1 class=\"headline__text inline-placeholder vossi-headline-text\" data-editable=\"headlineText\" id=\"maincontent\">\n",
      "      Beijing promised to ‘fight back’ over Taiwan leader’s US visit. But this time it has more to lose\n",
      "    </h1>\n",
      "headline: Beijing promised to ‘fight back’ over Taiwan leader’s US visit. But this time it has more to lose\n",
      "author_tag: <div class=\"byline__names\">\n",
      "\t\t\tAnalysis by <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/simone-mccarthy\"><span class=\"byline__name\">Simone McCarthy</span></a>\n",
      "</div>\n",
      "author_name: Simone McCarthy\n",
      "Failed to fetch full content from https://www.cnn.com/2023/04/04/asia/tsai-ing-wen-taiwan-mccarthy-meeting-analysis-intl-hnk/index.html: time data '3:49 AM Eastern, Tue April 4, 2023' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: <h1 class=\"headline__text inline-placeholder vossi-headline-text\" data-editable=\"headlineText\" id=\"maincontent\">\n",
      "      ‘Scary, cold, hungry and lonely’: Volunteer soldier shares experience on front line\n",
      "    </h1>\n",
      "headline: ‘Scary, cold, hungry and lonely’: Volunteer soldier shares experience on front line\n",
      "author_tag: None\n",
      "author_name: Author not found\n",
      "Failed to fetch full content from https://www.cnn.com/videos/world/2023/04/03/soldiers-russia-ukraine-war-david-mckenzie-pkg-ovn-intl-ldn-vpx.cnn: time data 'Published time not found' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: <h1 class=\"headline__text inline-placeholder vossi-headline-text\" data-editable=\"headlineText\" id=\"maincontent\">\n",
      "      The four astronauts NASA picked for the first crewed moon mission in 50 years\n",
      "    </h1>\n",
      "headline: The four astronauts NASA picked for the first crewed moon mission in 50 years\n",
      "author_tag: <div class=\"byline__names\">\n",
      "\t\t\tBy <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/jackie-wattles\"><span class=\"byline__name\">Jackie Wattles</span></a> and <a class=\"byline__link\" href=\"https://www.cnn.com/profiles/ashley-strickland-profile\"><span class=\"byline__name\">Ashley Strickland</span></a>, CNN\n",
      "\t\t</div>\n",
      "author_name: Jackie Wattles\n",
      "Failed to fetch full content from https://www.cnn.com/2023/04/03/world/artemis-2-astronaut-crew-scn/index.html: time data '7:44 AM Eastern, Tue April 4, 2023' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: <h1 class=\"headline__text inline-placeholder vossi-headline-text\" data-editable=\"headlineText\" id=\"maincontent\">\n",
      "      French minister under fire for Playboy magazine cover\n",
      "    </h1>\n",
      "headline: French minister under fire for Playboy magazine cover\n",
      "author_tag: <div class=\"byline__names\">\n",
      "\t\t\tBy <span class=\"byline__name\">Niamh Kennedy</span>, CNN\n",
      "\t\t</div>\n",
      "author_name: Niamh Kennedy\n",
      "Failed to fetch full content from https://www.cnn.com/2023/04/02/europe/french-minister-playboy-cover-intl/index.html: time data '7:54 AM Eastern, Mon April 3, 2023' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n",
      "headline_tag: <h1 class=\"headline__text inline-placeholder vossi-headline-text\" data-editable=\"headlineText\" id=\"maincontent\">\n",
      "      Why did Finland’s PM lose? Reporter explains the key issue voters cared about\n",
      "    </h1>\n",
      "headline: Why did Finland’s PM lose? Reporter explains the key issue voters cared about\n",
      "author_tag: None\n",
      "author_name: Author not found\n",
      "Failed to fetch full content from https://www.cnn.com/videos/world/2023/04/03/finland-prime-minister-sanna-marin-concedes-election-right-wing-party-wins-ovn-intl-ldn-vpx.cnn: time data 'Published time not found' does not match format '%I:%M %p %Z, %a %B %d, %Y'\n",
      "Assigned category: Top Stories\n",
      "Assigned category: Top Stories\n",
      "Portal: cnn\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    crawl_news()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
