{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0480b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: feedparser in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (6.0.11)\n",
      "Requirement already satisfied: pymongo in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.10.1)\n",
      "Requirement already satisfied: rapidfuzz in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: selenium in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.25.0)\n",
      "Requirement already satisfied: transformers in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: sgmllib3k in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: filelock in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: networkx in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from urllib3<3,>=1.21.1->requests) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/dibyendu/anaconda3/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dibyendu/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/dibyendu/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <F6236B89-E4CA-3330-B665-E463D537EAF3> /Users/dibyendu/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <A51C8C05-245A-3989-8D3C-9A6704422CA5> /Users/dibyendu/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 feedparser pymongo rapidfuzz selenium transformers torch\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from rapidfuzz import fuzz\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from transformers import pipeline\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edc099-067f-4beb-bd04-9305a27000e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39558760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Setup\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Update with your MongoDB connection URI\n",
    "db = client['news_database']\n",
    "collection = db['news_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d89ecf-cc73-417b-a42f-49a364314e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load a summarization model and sentiment analysis model\n",
    "summarizer = pipeline('summarization')\n",
    "sentiment_analyzer = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80bf74a-117c-4480-9f19-66ee61df9cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a722d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Selenium Chrome driver (headless mode)\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # Runs Chrome in headless mode\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    service = Service('/opt/homebrew/bin/chromedriver')  # Specify path to your ChromeDriver\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2043b9f-ce6c-4c65-abdb-6255db9013ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_summarize_text(text, max_len=50, min_len=20):\n",
    "    \"\"\"\n",
    "    Clean sensationalism and summarize the input text (title or description).\n",
    "    \"\"\"\n",
    "    # Summarize the input text (if needed)\n",
    "    summary = summarizer(text, max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Bolden important points in the summary\n",
    "    cleaned_summary = bolden_important_points(summary)\n",
    "    \n",
    "    return cleaned_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34205123-90f9-4b33-961a-b2c2416b6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all articles from the collection\n",
    "#articles = []\n",
    "#articles = list(collection.find())\n",
    "#mark_and_save_duplicate_articles();\n",
    "\n",
    "def clean_sensationalism(article_text):\n",
    "    \"\"\"\n",
    "    Cleans sensationalism by detecting overly emotional or sensational language \n",
    "    and rephrasing the text to focus on the facts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Break the text into sentences\n",
    "    if not article_text:  # Check if article_text is None or empty\n",
    "        return \"Content not available or cannot be cleaned.\"\n",
    "    \n",
    "    sentences = article_text.split('. ')\n",
    "    factual_sentences = []\n",
    "    print(f\"Sentences: {sentences}\")\n",
    "\n",
    "    # Analyze each sentence\n",
    "    for sentence in sentences:\n",
    "        sentiment = sentiment_analyzer(sentence)\n",
    "        if sentiment[0]['label'] in ['NEGATIVE', 'POSITIVE'] and sentiment[0]['score'] > 0.7:\n",
    "            # Skip overly emotional sentences or rewrite them\n",
    "            print(f\"Skipping sensational sentence: {sentence}\")\n",
    "            continue\n",
    "        factual_sentences.append(sentence)\n",
    "\n",
    "    # Join the factual sentences\n",
    "    clean_text = '. '.join(factual_sentences)\n",
    "    \n",
    "    # Use summarization to condense the cleaned text\n",
    "    summary = summarizer(clean_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a1effd-a359-4bd4-b5c6-27d21826cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relative_time_to_iso(relative_time):\n",
    "    \"\"\"\n",
    "    Convert a relative time string like '21 hours ago' to an ISO timestamp.\n",
    "    \"\"\"\n",
    "    # Get the current time\n",
    "    current_time = datetime.utcnow()\n",
    "\n",
    "    # Use regular expressions to extract time units and values\n",
    "    match = re.match(r\"(\\d+)\\s*(\\w+)\\s*ago\", relative_time)\n",
    "    if not match:\n",
    "        return None  # Return None if the format is not recognized\n",
    "\n",
    "    amount, unit = int(match.group(1)), match.group(2)\n",
    "\n",
    "    # Convert the time unit into a timedelta object\n",
    "    if \"hour\" in unit:\n",
    "        time_delta = timedelta(hours=amount)\n",
    "    elif \"minute\" in unit:\n",
    "        time_delta = timedelta(minutes=amount)\n",
    "    elif \"second\" in unit:\n",
    "        time_delta = timedelta(seconds=amount)\n",
    "    elif \"day\" in unit:\n",
    "        time_delta = timedelta(days=amount)\n",
    "    else:\n",
    "        return None  # If it's a unit we don't handle, return None\n",
    "\n",
    "    # Subtract the timedelta from the current time\n",
    "    calculated_time = current_time - time_delta\n",
    "\n",
    "    # Convert to ISO format\n",
    "    return calculated_time.isoformat() + \"Z\"  # Adding 'Z' to denote UTC time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b907dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch full article content\n",
    "def fetch_full_article(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        full_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "        return full_text if full_text else \"Full content not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch full content from {url}: {e}\")\n",
    "        return \"Failed to fetch full content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d39c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Google News (Top Stories and Local News)\n",
    "def scrape_google_news_1():\n",
    "    driver = setup_driver()\n",
    "\n",
    "    # Navigate to Google News\n",
    "    driver.get('https://news.google.com/home?hl=en-IN&gl=IN&ceid=IN:en')\n",
    "    time.sleep(5)  # Wait for page to load completely\n",
    "\n",
    "    news_data = []\n",
    "\n",
    "    # Scraping Top Stories\n",
    "    top_stories_section = driver.find_elements(By.CSS_SELECTOR, 'div.NiLAwe')[:5]  # Limiting to top 5 stories\n",
    "    print(top_stories_section)\n",
    "    for story in top_stories_section:\n",
    "        try:\n",
    "            headline = story.find_element(By.CSS_SELECTOR, 'h3').text\n",
    "            url = story.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
    "            source = story.find_element(By.CSS_SELECTOR, 'div.SVJrMe').text\n",
    "            full_url = f\"https://news.google.com{url[1:]}\"  # Append domain for full URL\n",
    "\n",
    "            # Fetch full news description\n",
    "            full_description = fetch_full_article(full_url)\n",
    "\n",
    "            cleaned_headline = clean_sensationalism(headline)\n",
    "            cleaned_description = clean_sensationalism(full_description)\n",
    "            #print(\"Cleaned and concise article: \")\n",
    "            print(f\"{cleaned_headline}\\n\")        \n",
    "            print(f\"{cleaned_description}\\n\")\n",
    "            \n",
    "\n",
    "            news_item = {\n",
    "                \"headline\": headline,\n",
    "                \"url\": full_url,\n",
    "                \"description\": full_description,\n",
    "                \"source\": source,\n",
    "                \"category\": \"Top Stories\",\n",
    "                \"cleaned_headline\": cleaned_headline,\n",
    "                \"cleaned_description\": cleaned_description\n",
    "            }\n",
    "            print(news_item)  # Print to console\n",
    "            news_data.append(news_item)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching top story: {e}\")\n",
    "    # Scraping Local News (Use similar method to scrape local news)\n",
    "    local_news_section = driver.find_elements(By.CSS_SELECTOR, 'div.NiLAwe')[:5]  # Limiting to top 5 local news\n",
    "    for story in local_news_section:\n",
    "        try:\n",
    "            headline = story.find_element(By.CSS_SELECTOR, 'h3').text\n",
    "            url = story.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
    "            source = story.find_element(By.CSS_SELECTOR, 'div.SVJrMe').text\n",
    "            full_url = f\"https://news.google.com{url[1:]}\"  # Append domain for full URL\n",
    "\n",
    "            # Fetch full news description\n",
    "            full_description = fetch_full_article(full_url)\n",
    "\n",
    "            news_item = {\n",
    "                \"headline\": headline,\n",
    "                \"url\": full_url,\n",
    "                \"description\": full_description,\n",
    "                \"source\": source,\n",
    "                \"category\": \"Local News\"\n",
    "            }\n",
    "            print(news_item)  # Print to console\n",
    "            news_data.append(news_item)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching local news: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return news_data            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b099d752-d471-49d7-9294-0cdec111d92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 article links:\n"
     ]
    }
   ],
   "source": [
    "def scrape_article_links():\n",
    "    \"\"\"\n",
    "    Scrape article links from Google News homepage.\n",
    "    \"\"\"\n",
    "    driver = setup_driver()\n",
    "    driver.get('https://news.google.com/home?hl=en-IN&gl=IN&ceid=IN:en')\n",
    "\n",
    "    time.sleep(5)  # Allow page to load completely\n",
    "\n",
    "    try:\n",
    "        # Find all <article> tags on the page\n",
    "        articles = driver.find_elements(By.TAG_NAME, 'article')\n",
    "        article_links = []\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract the <a> tag within each article\n",
    "                link_element = article.find_element(By.TAG_NAME, 'a')\n",
    "                link = link_element.get_attribute('href')\n",
    "                if link and link not in article_links:\n",
    "                    article_links.append(link)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting link from article: {e}\")\n",
    "\n",
    "        # Print the list of links\n",
    "        print(f\"Found {len(article_links)} article links:\")\n",
    "        for link in article_links:\n",
    "            print(link)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_article_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc400153-0327-4706-b489-b7ba230e4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google_news():\n",
    "    driver = setup_driver()\n",
    "\n",
    "    # Navigate to Google News\n",
    "    driver.get('https://news.google.com/home?hl=en-IN&gl=IN&ceid=IN:en')\n",
    "\n",
    "    # Wait for the page to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # Locate Top Stories\n",
    "        top_stories_section = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.NiLAwe')))\n",
    "        news_data = []\n",
    "\n",
    "        for story in top_stories_section[:5]:  # Limit to top 5 stories\n",
    "            try:\n",
    "                headline = story.find_element(By.CSS_SELECTOR, 'h3').text\n",
    "                url = story.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
    "                source = story.find_element(By.CSS_SELECTOR, 'div.SVJrMe').text\n",
    "                full_url = f\"https://news.google.com{url[1:]}\"  # Construct full URL\n",
    "\n",
    "                news_item = {\n",
    "                    \"headline\": headline,\n",
    "                    \"url\": full_url,\n",
    "                    \"source\": source,\n",
    "                    \"category\": \"Top Stories\"\n",
    "                }\n",
    "                print(news_item)  # Print each story to verify\n",
    "                news_data.append(news_item)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching story: {e}\")\n",
    "\n",
    "        driver.quit()\n",
    "        return news_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        driver.quit()\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "934b4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save news data to MongoDB\n",
    "def save_to_mongodb(news_data):\n",
    "    if news_data:\n",
    "        collection.insert_many(news_data)\n",
    "        print(\"News saved to MongoDB\")\n",
    "    else:\n",
    "        print(\"No news to save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d019870-9a0c-4943-bed3-99f37e645575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3311208-14a7-4269-a962-39649582a1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3423f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Message: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    #news_data = scrape_google_news()\n",
    "    #save_to_mongodb(news_data)\n",
    "    scrape_news_links()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
