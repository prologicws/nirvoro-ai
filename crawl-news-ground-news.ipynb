{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b21264-c79d-4e71-a89d-f65bef8e0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c0f0ad-403d-44eb-a950-d648497a6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Selenium Chrome driver (headless mode)\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # Runs Chrome in headless mode\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    service = Service('/opt/homebrew/bin/chromedriver')  # Specify path to your ChromeDriver\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c5f511-b15a-4dc7-848f-597943405592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver\n",
    "driver = setup_driver()\n",
    "# Open Ground News\n",
    "url = \"https://ground.news\"\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Allow the page to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d0945da-c056-49e7-a81a-a0bceb3a2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scroll to load all categories\n",
    "scroll_pause_time = 3\n",
    "num_scrolls = 5  # Adjust as needed\n",
    "\n",
    "for _ in range(num_scrolls):\n",
    "    driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)  # Scroll to the bottom\n",
    "    time.sleep(scroll_pause_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff7161c-3483-427d-b2bb-d60d8c598f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news links found. Please check the structure of the website.\n"
     ]
    }
   ],
   "source": [
    "# Parse the page source\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Extract article links\n",
    "article_links = []\n",
    "for a_tag in soup.find_all(\"a\", href=True):\n",
    "    href = a_tag[\"href\"]\n",
    "    if \"/daily-briefing/\" in href and not href.startswith(\"#\"):  # Filter for article links\n",
    "        full_link = href if href.startswith(\"http\") else f\"https://ground.news{href}\"\n",
    "        if full_link not in article_links:  # Avoid duplicates\n",
    "            article_links.append(full_link)\n",
    "\n",
    "# Print all collected links\n",
    "if article_links:\n",
    "    print(\"News Links Found:\")\n",
    "    for link in article_links:\n",
    "        print(link)\n",
    "else:\n",
    "    print(\"No news links found. Please check the structure of the website.\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb13c8-7383-432f-b8cf-19e13593ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all collected links\n",
    "print(\"News Links Found:\")\n",
    "for link in article_links:\n",
    "    print(link)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d141da-473a-4731-9566-43306a319f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll and Load Articles\n",
    "scroll_pause_time = 3\n",
    "num_scrolls = 15  # Adjust the number of scrolls as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fc21a-8fe5-4965-9c77-12c8ac81ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visit each category link and scrape news items\n",
    "for category_link in category_links:\n",
    "    driver.get(category_link)\n",
    "    time.sleep(5)  # Allow category page to load\n",
    "\n",
    "    # Scroll to load more news articles on the category page\n",
    "    for _ in range(5):  # Adjust number of scrolls per category\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "    # Parse the category page for news articles\n",
    "    category_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = category_soup.find_all(\"div\", class_=\"card-container\")  # Adjust class to match news items\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title = article.find(\"h3\").get_text(strip=True)  # Adjust tag for titles\n",
    "            link = article.find(\"a\", href=True)[\"href\"]  # Extract article link\n",
    "            if not link.startswith(\"http\"):\n",
    "                link = f\"https://ground.news{link}\"\n",
    "            sentiment_score = random.randint(1, 5)  # Assign random sentiment score\n",
    "            all_news.append({\"title\": title, \"link\": link, \"sentiment_score\": sentiment_score})\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting news article: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3761bbb-4c4e-4b65-b66e-88bc0d6a2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV\n",
    "if all_news:\n",
    "    df = pd.DataFrame(all_news)\n",
    "    df.to_csv(\"ground_news_actual_articles.csv\", index=False)\n",
    "    print(f\"Saved {len(all_news)} articles to ground_news_actual_articles.csv\")\n",
    "else:\n",
    "    print(\"No articles found. Please check the structure of the website.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd718cd4-df6e-4980-adbc-84a55fad8e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c365c49-0520-4045-a279-6f66626f8474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b634b-0aeb-4118-ac4d-51ce1d6f8d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
